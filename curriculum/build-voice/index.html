<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Build Your Own Voice Assistant ‚Äî From Scratch</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #fafafa;
            color: #1a1a2e;
            font-size: 18px;
            line-height: 1.7;
        }

        .container {
            max-width: 720px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        .hero {
            text-align: center;
            margin-bottom: 80px;
        }

        .hero h1 {
            font-size: 3rem;
            font-weight: 600;
            margin-bottom: 40px;
            line-height: 1.2;
        }

        .hero p {
            font-size: 1.2rem;
            margin-bottom: 20px;
            color: #4a5568;
        }

        .chapter {
            margin-bottom: 120px;
            opacity: 0;
            transform: translateY(40px);
            transition: all 0.8s ease;
        }

        .chapter.visible {
            opacity: 1;
            transform: translateY(0);
        }

        .chapter h2 {
            font-size: 2.5rem;
            margin-bottom: 40px;
            color: #1a1a2e;
        }

        .chapter h3 {
            font-size: 1.5rem;
            margin: 40px 0 20px 0;
            color: #2563eb;
        }

        .card {
            background: white;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            margin: 30px 0;
            border: 1px solid #e2e8f0;
        }

        .code-block {
            background: #f1f5f9;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            position: relative;
            overflow-x: auto;
        }

        .code-block pre {
            margin: 0;
            font-family: 'Monaco', 'Menlo', monospace;
            font-size: 14px;
            line-height: 1.4;
        }

        .copy-btn {
            position: absolute;
            top: 15px;
            right: 15px;
            background: #2563eb;
            color: white;
            border: none;
            padding: 8px 12px;
            border-radius: 6px;
            font-size: 12px;
            cursor: pointer;
            transition: background-color 0.2s;
        }

        .copy-btn:hover {
            background: #1d4ed8;
        }

        .run-btn {
            background: #16a34a;
            color: white;
            border: none;
            padding: 12px 24px;
            border-radius: 8px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            margin: 20px 0;
            transition: all 0.2s;
        }

        .run-btn:hover {
            background: #15803d;
            transform: translateY(-1px);
        }

        .audio-demo {
            background: #fefce8;
            border: 2px solid #eab308;
            border-radius: 12px;
            padding: 20px;
            margin: 20px 0;
            text-align: center;
        }

        .audio-controls {
            display: flex;
            gap: 15px;
            justify-content: center;
            align-items: center;
            margin: 20px 0;
        }

        .record-btn {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            background: #dc2626;
            color: white;
            border: none;
            font-size: 24px;
            cursor: pointer;
            transition: all 0.3s ease;
            position: relative;
        }

        .record-btn:hover {
            transform: scale(1.1);
            box-shadow: 0 0 20px rgba(220, 38, 38, 0.5);
        }

        .record-btn.recording {
            animation: pulse 1s infinite;
        }

        .record-btn.listening {
            background: #16a34a;
        }

        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.1); }
            100% { transform: scale(1); }
        }

        .waveform {
            display: flex;
            align-items: end;
            justify-content: center;
            height: 100px;
            gap: 2px;
            margin: 20px 0;
        }

        .waveform-bar {
            width: 4px;
            background: #2563eb;
            border-radius: 2px;
            transition: height 0.1s ease;
        }

        .spectrogram {
            width: 100%;
            height: 150px;
            background: linear-gradient(45deg, #1e40af, #3b82f6, #60a5fa);
            border-radius: 8px;
            margin: 20px 0;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: 600;
            position: relative;
            overflow: hidden;
        }

        .spectrogram::before {
            content: '';
            position: absolute;
            top: 0;
            left: -100%;
            width: 100%;
            height: 100%;
            background: linear-gradient(90deg, transparent, rgba(255,255,255,0.3), transparent);
            animation: scan 3s infinite linear;
        }

        @keyframes scan {
            0% { left: -100%; }
            100% { left: 100%; }
        }

        .pipeline-flow {
            display: flex;
            flex-direction: column;
            gap: 20px;
            align-items: center;
            margin: 40px 0;
            padding: 40px;
            background: white;
            border-radius: 12px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        }

        .pipeline-node {
            background: #2563eb;
            color: white;
            padding: 15px 25px;
            border-radius: 8px;
            font-weight: 600;
            text-align: center;
            min-width: 200px;
            transition: all 0.3s ease;
        }

        .pipeline-node.active {
            background: #16a34a;
            transform: scale(1.05);
            box-shadow: 0 0 20px rgba(22, 163, 74, 0.5);
        }

        .pipeline-arrow {
            font-size: 24px;
            color: #6b7280;
        }

        .voice-test {
            background: #f0fdf4;
            border: 2px solid #16a34a;
            border-radius: 12px;
            padding: 20px;
            margin: 20px 0;
        }

        .voice-selector {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }

        .voice-option {
            background: white;
            border: 2px solid #e2e8f0;
            border-radius: 8px;
            padding: 15px;
            text-align: center;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .voice-option:hover {
            border-color: #2563eb;
            transform: translateY(-2px);
        }

        .voice-option.selected {
            border-color: #2563eb;
            background: #f0f9ff;
        }

        .voice-conversation {
            background: #f8fafc;
            border-radius: 12px;
            padding: 20px;
            margin: 20px 0;
            max-height: 300px;
            overflow-y: auto;
        }

        .conversation-msg {
            margin-bottom: 15px;
            padding: 12px 16px;
            border-radius: 12px;
            max-width: 80%;
        }

        .user-voice {
            background: #2563eb;
            color: white;
            margin-left: auto;
            text-align: right;
        }

        .ai-voice {
            background: #16a34a;
            color: white;
        }

        .processing {
            background: #f59e0b;
            color: white;
            font-style: italic;
            text-align: center;
            margin: 0 auto;
            max-width: 200px;
        }

        .talk-button {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            background: linear-gradient(135deg, #2563eb, #1d4ed8);
            color: white;
            border: none;
            font-size: 18px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            position: relative;
            box-shadow: 0 8px 25px rgba(37, 99, 235, 0.3);
        }

        .talk-button:hover {
            transform: scale(1.1);
            box-shadow: 0 12px 35px rgba(37, 99, 235, 0.5);
        }

        .talk-button.active {
            background: linear-gradient(135deg, #dc2626, #b91c1c);
            animation: glow 2s infinite;
        }

        @keyframes glow {
            0%, 100% { box-shadow: 0 0 20px rgba(220, 38, 38, 0.5); }
            50% { box-shadow: 0 0 40px rgba(220, 38, 38, 0.8); }
        }

        .architecture-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 30px 0;
        }

        .before-after {
            background: white;
            padding: 20px;
            border-radius: 12px;
            border: 2px solid #e2e8f0;
        }

        .before-after.before {
            border-color: #dc2626;
            background: #fef2f2;
        }

        .before-after.after {
            border-color: #16a34a;
            background: #f0fdf4;
        }

        .go-deeper {
            background: #f0f9ff;
            border-left: 4px solid #2563eb;
            padding: 15px 20px;
            margin: 20px 0;
        }

        .go-deeper h4 {
            color: #2563eb;
            margin-bottom: 10px;
        }

        .go-deeper a {
            color: #2563eb;
            text-decoration: none;
        }

        .go-deeper a:hover {
            text-decoration: underline;
        }

        .cta-section {
            background: linear-gradient(135deg, #2563eb, #1d4ed8);
            color: white;
            padding: 60px 40px;
            border-radius: 20px;
            text-align: center;
            margin: 60px 0;
        }

        .cta-section h3 {
            font-size: 2rem;
            margin-bottom: 20px;
            color: white;
        }

        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .feature-card {
            background: rgba(255,255,255,0.1);
            padding: 20px;
            border-radius: 10px;
            text-align: center;
        }

        @media (max-width: 768px) {
            .hero h1 {
                font-size: 2rem;
            }

            .chapter h2 {
                font-size: 2rem;
            }

            .architecture-comparison {
                grid-template-columns: 1fr;
            }

            .container {
                padding: 20px 15px;
            }

            .talk-button {
                width: 100px;
                height: 100px;
                font-size: 16px;
            }
        }

        .api-fallback {
            background: #fef3c7;
            border: 2px solid #f59e0b;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
        }

        .audio-player {
            background: #f3f4f6;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
            display: flex;
            align-items: center;
            gap: 15px;
        }

        .play-btn {
            width: 40px;
            height: 40px;
            border-radius: 50%;
            background: #2563eb;
            color: white;
            border: none;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
        }
    </style>
</head>
<body>
    <div style="padding:12px 20px;font-size:14px;color:#64748b;max-width:720px;margin:0 auto;">
        <a href="../" style="color:#2563eb;text-decoration:none;">‚Üê Back to Series</a>
        <span style="float:right;">Course 6 of 14</span>
    </div>
    <div class="container">
        <div class="hero">
            <h1>Build Your Own Voice Assistant</h1>
            <p>You've been typing to AI. What if you could talk to it?</p>
            <p><strong>In 30 minutes you'll build a voice assistant from microphone to speaker.</strong></p>
            <p>For absolute beginners. No assumed knowledge.</p>
        </div>

        <!-- Chapter 1: Sound is Numbers -->
        <div class="chapter visible" id="chapter-1">
            <h2>Chapter 1: Sound is Numbers</h2>
            
            <p>What is audio? <strong>Your voice is just a list of numbers, 44,100 per second.</strong></p>

            <div class="card">
                <h3>Record and Visualize</h3>
                <div class="audio-demo">
                    <p>Click the red button to record 3 seconds of audio:</p>
                    <div class="audio-controls">
                        <button class="record-btn" id="record-btn" onclick="startRecording()">üé§</button>
                        <div id="record-status">Ready to record</div>
                    </div>
                    
                    <div class="waveform" id="waveform" style="display: none;">
                        <!-- Waveform bars will be generated here -->
                    </div>
                    
                    <div class="spectrogram" id="spectrogram" style="display: none;">
                        Frequency spectrum of your voice
                    </div>
                    
                    <div id="audio-data" style="display: none; margin-top: 20px; font-family: monospace; font-size: 12px; background: #f3f4f6; padding: 15px; border-radius: 8px; max-height: 100px; overflow-y: auto;">
                        <!-- Raw audio numbers will appear here -->
                    </div>
                </div>
                
                <p>When you speak, your vocal cords vibrate air molecules. A microphone converts those pressure waves into electrical signals. <strong>An analog-to-digital converter samples that signal 44,100 times per second.</strong></p>
                
                <p>Each sample is a number between -1 and 1. Your entire voice ‚Äî every word, every tone, every breath ‚Äî is just a sequence of numbers.</p>
            </div>

            <div class="card">
                <h3>The Raw Data</h3>
                <div class="code-block">
                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                    <pre><code>// Audio recording in JavaScript
async function recordAudio() {
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  const mediaRecorder = new MediaRecorder(stream);
  const chunks = [];
  
  mediaRecorder.ondataavailable = (event) => {
    chunks.push(event.data);
  };
  
  mediaRecorder.onstop = async () => {
    const blob = new Blob(chunks, { type: 'audio/wav' });
    const arrayBuffer = await blob.arrayBuffer();
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    
    // This is your voice as numbers!
    const samples = audioBuffer.getChannelData(0); // Float32Array
    console.log(`Recorded ${samples.length} samples`);
    console.log(`First 10 samples: ${Array.from(samples.slice(0, 10))}`);
    // [0.0234, -0.0456, 0.0123, -0.0789, ...]
  };
  
  mediaRecorder.start();
  setTimeout(() => mediaRecorder.stop(), 3000); // Record for 3 seconds
}</code></pre>
                </div>
                <button class="run-btn" onclick="enableAudioCapture()">Enable Audio Capture</button>
            </div>

            <div class="go-deeper">
                <h4>üìö Go Deeper</h4>
                <p><a href="https://en.wikipedia.org/wiki/Digital_audio" target="_blank">Digital Audio on Wikipedia</a> ‚Äî How sound becomes numbers</p>
            </div>
        </div>

        <!-- Chapter 2: Speech to Text -->
        <div class="chapter" id="chapter-2">
            <h2>Chapter 2: Speech to Text</h2>
            
            <p><strong>OpenAI's Whisper does this in 50 languages.</strong> Whisper architecture: audio spectrogram ‚Üí encoder ‚Üí decoder ‚Üí text.</p>

            <div class="card">
                <h3>The Whisper Pipeline</h3>
                <div class="pipeline-flow">
                    <div class="pipeline-node" id="pipe-audio">Audio Waveform</div>
                    <div class="pipeline-arrow">‚Üì</div>
                    <div class="pipeline-node" id="pipe-spec">Mel Spectrogram</div>
                    <div class="pipeline-arrow">‚Üì</div>
                    <div class="pipeline-node" id="pipe-encoder">Transformer Encoder</div>
                    <div class="pipeline-arrow">‚Üì</div>
                    <div class="pipeline-node" id="pipe-decoder">Transformer Decoder</div>
                    <div class="pipeline-arrow">‚Üì</div>
                    <div class="pipeline-node" id="pipe-text">Text Output</div>
                </div>
                <button class="run-btn" onclick="animatePipeline()">Animate Pipeline</button>
            </div>

            <div class="card">
                <h3>Test Speech Recognition</h3>
                <div class="voice-test">
                    <p>Try it yourself ‚Äî speak into your microphone:</p>
                    <div class="audio-controls">
                        <button class="record-btn listening" id="listen-btn" onclick="startListening()">üéß</button>
                        <div id="listen-status">Click to start listening</div>
                    </div>
                    
                    <div id="transcription" style="margin-top: 20px; padding: 15px; background: white; border-radius: 8px; min-height: 50px; font-style: italic; color: #6b7280; display: none;">
                        Transcription will appear here...
                    </div>
                    
                    <div id="word-timing" style="display: none; margin-top: 15px;">
                        <h4>Word-level Timestamps</h4>
                        <div id="timing-display" style="font-family: monospace; font-size: 14px; background: #f8fafc; padding: 10px; border-radius: 6px;">
                            <!-- Word timings will appear here -->
                        </div>
                    </div>
                </div>
                
                <div class="api-fallback">
                    <strong>No API key?</strong> We'll use your browser's built-in SpeechRecognition API as fallback. It's not as good as Whisper, but it works!
                </div>
            </div>

            <div class="card">
                <h3>The Code</h3>
                <div class="code-block">
                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                    <pre><code>// Speech-to-text with OpenAI Whisper API
async function transcribeAudio(audioBlob) {
  const formData = new FormData();
  formData.append('file', audioBlob, 'audio.wav');
  formData.append('model', 'whisper-1');
  formData.append('response_format', 'verbose_json'); // Get word timestamps
  
  const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${OPENAI_API_KEY}`
    },
    body: formData
  });
  
  const result = await response.json();
  
  return {
    text: result.text,
    words: result.words, // [{word: "hello", start: 0.5, end: 0.9}, ...]
    language: result.language
  };
}

// Fallback: Browser SpeechRecognition API
function browserSpeechRecognition() {
  const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
  recognition.continuous = false;
  recognition.interimResults = false;
  
  return new Promise((resolve, reject) => {
    recognition.onresult = (event) => {
      const transcript = event.results[0][0].transcript;
      resolve({ text: transcript, confidence: event.results[0][0].confidence });
    };
    
    recognition.onerror = reject;
    recognition.start();
  });
}</code></pre>
                </div>
                <button class="run-btn" onclick="enableSpeechRecognition()">Enable Speech Recognition</button>
            </div>

            <div class="go-deeper">
                <h4>üìö Go Deeper</h4>
                <p><a href="https://arxiv.org/abs/2212.04356" target="_blank">Whisper Paper</a> ‚Äî "Robust Speech Recognition via Large-Scale Weak Supervision"</p>
                <p><a href="https://platform.openai.com/docs/guides/speech-to-text" target="_blank">OpenAI Whisper API</a> ‚Äî Complete documentation</p>
            </div>
        </div>

        <!-- Chapter 3: The Brain in the Middle -->
        <div class="chapter" id="chapter-3">
            <h2>Chapter 3: The Brain in the Middle</h2>
            
            <p>You have text from speech. Send it to an LLM. Get a text response. <strong>The LLM doesn't know it's talking to a human. It just sees text.</strong></p>

            <div class="card">
                <h3>Voice ‚Üí Text ‚Üí LLM ‚Üí Text</h3>
                <div class="architecture-comparison">
                    <div class="before-after before">
                        <h4>What the Human Says</h4>
                        <div style="background: white; padding: 15px; margin: 10px 0; border-radius: 8px;">
                            üé§ "What's the weather like today?"
                        </div>
                    </div>
                    <div class="before-after after">
                        <h4>What the LLM Sees</h4>
                        <div style="background: white; padding: 15px; margin: 10px 0; border-radius: 8px; font-family: monospace;">
                            "What's the weather like today?"
                        </div>
                    </div>
                </div>
                
                <p><strong>This is identical to any text-based AI agent.</strong> Tools, memory, reasoning ‚Äî everything works the same. The only difference is how the input arrived and how the output will be delivered.</p>

                <div style="margin:20px 0;padding:16px 20px;background:#eff6ff;border-left:4px solid #2563eb;border-radius:0 8px 8px 0;font-size:15px;color:#1e40af;">
                  üîó <strong>Connection:</strong> This is just the Agent course in disguise. Speech comes in, gets converted to text, hits the same LLM + tools + memory stack you've already built, and the response gets converted back to speech. The 'brain' is everything you learned in courses 1-7.
                </div>
            </div>

            <div class="card">
                <h3>Test Voice + AI</h3>
                <div class="voice-conversation" id="voice-conversation" style="display: none;">
                    <!-- Conversation messages will appear here -->
                </div>
                
                <div class="audio-controls">
                    <button class="talk-button" id="talk-btn" onclick="startVoiceChat()">
                        üé§<br>
                        <small>Press & Hold<br>to Talk</small>
                    </button>
                </div>
                
                <div id="voice-status">Click and hold the button to talk to your AI assistant</div>
            </div>

            <div class="card">
                <h3>The Code</h3>
                <div class="code-block">
                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                    <pre><code>// Complete voice ‚Üí AI ‚Üí voice pipeline
class VoiceAssistant {
  constructor(apiKey) {
    this.apiKey = apiKey;
    this.isRecording = false;
    this.mediaRecorder = null;
  }
  
  async startVoiceChat() {
    if (this.isRecording) return;
    
    try {
      // 1. Start recording
      const audioBlob = await this.recordAudio();
      
      // 2. Transcribe speech to text
      const transcription = await this.transcribeAudio(audioBlob);
      console.log('User said:', transcription.text);
      
      // 3. Send to LLM
      const aiResponse = await this.callLLM(transcription.text);
      console.log('AI responded:', aiResponse);
      
      // 4. Convert response to speech (next chapter!)
      await this.speakText(aiResponse);
      
    } catch (error) {
      console.error('Voice chat error:', error);
    }
  }
  
  async callLLM(userText) {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [{
          role: 'user',
          content: userText
        }]
      })
    });
    
    const data = await response.json();
    return data.choices[0].message.content;
  }
}

const assistant = new VoiceAssistant(API_KEY);</code></pre>
                </div>
                <button class="run-btn" onclick="enableVoiceChat()">Enable Voice Chat</button>
            </div>

            <div class="go-deeper">
                <h4>üìö Go Deeper</h4>
                <p><a href="https://platform.openai.com/docs/guides/text-generation" target="_blank">OpenAI Chat API</a> ‚Äî Same API you'd use for text chat</p>
            </div>
        </div>

        <!-- Chapter 4: Text to Speech -->
        <div class="chapter" id="chapter-4">
            <h2>Chapter 4: Text to Speech</h2>
            
            <p>Turn the response back into audio. <strong>Text ‚Üí phonemes ‚Üí mel spectrogram ‚Üí waveform.</strong></p>

            <div class="card">
                <h3>Choose Your Voice</h3>
                <div class="voice-selector" id="voice-selector">
                    <div class="voice-option" onclick="selectVoice('alloy')" data-voice="alloy">
                        ü§ñ<br><strong>Alloy</strong><br><small>Neutral, clear</small>
                    </div>
                    <div class="voice-option" onclick="selectVoice('echo')" data-voice="echo">
                        üé≠<br><strong>Echo</strong><br><small>Expressive, dynamic</small>
                    </div>
                    <div class="voice-option" onclick="selectVoice('nova')" data-voice="nova">
                        ‚≠ê<br><strong>Nova</strong><br><small>Warm, engaging</small>
                    </div>
                    <div class="voice-option selected" onclick="selectVoice('onyx')" data-voice="onyx">
                        üé§<br><strong>Onyx</strong><br><small>Deep, authoritative</small>
                    </div>
                </div>
                
                <div style="margin: 20px 0;">
                    <input type="text" id="tts-text" placeholder="Enter text to speak..." style="width: 100%; padding: 12px; border: 1px solid #e2e8f0; border-radius: 8px;" />
                    <button class="run-btn" onclick="testTextToSpeech()">üîä Speak This Text</button>
                </div>
                
                <div id="tts-result" class="audio-player" style="display: none;">
                    <button class="play-btn" id="play-btn" onclick="playGeneratedAudio()">‚ñ∂</button>
                    <div>
                        <strong>Generated Speech</strong><br>
                        <small>Click play to hear your AI's voice</small>
                    </div>
                </div>
            </div>

            <div class="card">
                <h3>TTS Pipeline</h3>
                <div class="pipeline-flow">
                    <div class="pipeline-node">Text: "Hello, world!"</div>
                    <div class="pipeline-arrow">‚Üì</div>
                    <div class="pipeline-node">Phonemes: /h…ôÀàlo ä w…úÀêrld/</div>
                    <div class="pipeline-arrow">‚Üì</div>
                    <div class="pipeline-node">Mel Spectrogram</div>
                    <div class="pipeline-arrow">‚Üì</div>
                    <div class="pipeline-node">Audio Waveform</div>
                </div>
                
                <p><strong>Different approaches to TTS:</strong></p>
                <ul style="margin: 15px 0; padding-left: 30px;">
                    <li><strong>OpenAI TTS:</strong> Neural TTS with 6 voices</li>
                    <li><strong>ElevenLabs:</strong> Voice cloning and custom voices</li>
                    <li><strong>Browser SpeechSynthesis:</strong> Built-in, works offline</li>
                    <li><strong>Google Cloud TTS:</strong> 300+ voices, many languages</li>
                </ul>
            </div>

            <div class="card">
                <h3>The Code</h3>
                <div class="code-block">
                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                    <pre><code>// Text-to-speech with OpenAI TTS API
async function textToSpeech(text, voice = 'onyx') {
  const response = await fetch('https://api.openai.com/v1/audio/speech', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${OPENAI_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: 'tts-1',
      input: text,
      voice: voice, // alloy, echo, fable, onyx, nova, shimmer
      response_format: 'mp3'
    })
  });
  
  const audioBlob = await response.blob();
  return audioBlob;
}

// Fallback: Browser SpeechSynthesis API
function browserTextToSpeech(text) {
  return new Promise((resolve) => {
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.onend = resolve;
    
    // Get available voices
    const voices = speechSynthesis.getVoices();
    utterance.voice = voices.find(voice => voice.name.includes('Google')) || voices[0];
    
    speechSynthesis.speak(utterance);
  });
}

// Play audio blob
function playAudio(audioBlob) {
  const audioUrl = URL.createObjectURL(audioBlob);
  const audio = new Audio(audioUrl);
  audio.play();
  
  audio.onended = () => {
    URL.revokeObjectURL(audioUrl);
  };
}</code></pre>
                </div>
                <button class="run-btn" onclick="enableTextToSpeech()">Enable Text-to-Speech</button>
            </div>

            <div class="go-deeper">
                <h4>üìö Go Deeper</h4>
                <p><a href="https://platform.openai.com/docs/guides/text-to-speech" target="_blank">OpenAI TTS Guide</a> ‚Äî Complete TTS documentation</p>
                <p><a href="https://elevenlabs.io/docs" target="_blank">ElevenLabs API</a> ‚Äî Advanced voice cloning and synthesis</p>
            </div>
        </div>

        <!-- Chapter 5: The Full Loop -->
        <div class="chapter" id="chapter-5">
            <h2>Chapter 5: The Full Loop</h2>
            
            <p><strong>Mic ‚Üí Whisper ‚Üí LLM ‚Üí TTS ‚Üí Speaker.</strong> All connected. You just built Alexa. Siri. Every voice assistant works exactly like this.</p>

            <div class="card">
                <h3>Complete Voice Assistant</h3>
                <div class="voice-conversation" id="full-conversation">
                    <div class="conversation-msg ai-voice">
                        üëã Hi! I'm your voice assistant. Hold the button below to talk to me.
                    </div>
                </div>
                
                <div class="audio-controls" style="margin: 30px 0;">
                    <button class="talk-button" id="full-talk-btn" onmousedown="startFullVoiceChat()" onmouseup="endFullVoiceChat()" ontouchstart="startFullVoiceChat()" ontouchend="endFullVoiceChat()">
                        <div id="talk-btn-text">üé§<br>Hold to Talk</div>
                    </button>
                </div>
                
                <div id="full-status">Ready for voice conversation</div>
            </div>

            <div class="card">
                <h3>The Architecture You Built</h3>
                <div class="pipeline-flow">
                    <div class="pipeline-node">üé§ Microphone</div>
                    <div class="pipeline-arrow">‚Üì</div>
                    <div class="pipeline-node">üéß Whisper (Speech‚ÜíText)</div>
                    <div class="pipeline-arrow">‚Üì</div>
                    <div class="pipeline-node">üß† LLM (Think & Respond)</div>
                    <div class="pipeline-arrow">‚Üì</div>
                    <div class="pipeline-node">üîä TTS (Text‚ÜíSpeech)</div>
                    <div class="pipeline-arrow">‚Üì</div>
                    <div class="pipeline-node">üì¢ Speaker</div>
                </div>
                
                <p><strong>This is identical to every voice assistant:</strong></p>
                <ul style="margin: 15px 0; padding-left: 30px;">
                    <li>Amazon Alexa: Same pipeline, different models</li>
                    <li>Apple Siri: Same pipeline, different models</li>
                    <li>Google Assistant: Same pipeline, different models</li>
                    <li>Your assistant: Same pipeline, your choice of models!</li>
                </ul>
            </div>

            <div class="card">
                <h3>Complete Implementation</h3>
                <div class="code-block">
                    <button class="copy-btn" onclick="copyCode(this)">Copy</button>
                    <pre><code>// Complete voice assistant implementation
class FullVoiceAssistant {
  constructor() {
    this.isListening = false;
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.conversation = [];
  }
  
  async startListening() {
    if (this.isListening) return;
    this.isListening = true;
    
    try {
      // Get microphone access
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      this.mediaRecorder = new MediaRecorder(stream);
      this.audioChunks = [];
      
      this.mediaRecorder.ondataavailable = (event) => {
        this.audioChunks.push(event.data);
      };
      
      this.mediaRecorder.start();
      this.updateUI('recording', 'üé§ Listening...');
      
    } catch (error) {
      console.error('Failed to start recording:', error);
      this.updateUI('error', 'Microphone access denied');
    }
  }
  
  async stopListening() {
    if (!this.isListening) return;
    this.isListening = false;
    
    return new Promise(async (resolve) => {
      this.mediaRecorder.onstop = async () => {
        try {
          // 1. Process audio
          const audioBlob = new Blob(this.audioChunks, { type: 'audio/wav' });
          this.updateUI('processing', 'üéß Understanding...');
          
          // 2. Speech to text
          const transcription = await this.transcribe(audioBlob);
          this.addMessage('user', transcription.text);
          
          // 3. Get AI response
          this.updateUI('thinking', 'üß† Thinking...');
          const response = await this.getAIResponse(transcription.text);
          this.addMessage('assistant', response);
          
          // 4. Text to speech
          this.updateUI('speaking', 'üó£Ô∏è Speaking...');
          await this.speak(response);
          
          this.updateUI('ready', 'Hold to talk');
          resolve();
          
        } catch (error) {
          console.error('Processing error:', error);
          this.updateUI('error', 'Something went wrong');
        }
      };
      
      this.mediaRecorder.stop();
      // Stop all audio tracks
      this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
    });
  }
  
  async transcribe(audioBlob) {
    // Try OpenAI Whisper API first, fallback to browser
    try {
      const formData = new FormData();
      formData.append('file', audioBlob, 'audio.wav');
      formData.append('model', 'whisper-1');
      
      const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: { 'Authorization': `Bearer ${OPENAI_API_KEY}` },
        body: formData
      });
      
      const result = await response.json();
      return { text: result.text };
      
    } catch (error) {
      // Fallback to browser speech recognition
      return await this.browserSpeechRecognition();
    }
  }
  
  async getAIResponse(userText) {
    this.conversation.push({ role: 'user', content: userText });
    
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${OPENAI_API_KEY}`
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          {role: 'system', content: 'You are a helpful voice assistant. Keep responses concise and conversational.'},
          ...this.conversation
        ]
      })
    });
    
    const data = await response.json();
    const aiResponse = data.choices[0].message.content;
    this.conversation.push({ role: 'assistant', content: aiResponse });
    
    return aiResponse;
  }
  
  async speak(text) {
    try {
      // Try OpenAI TTS API first
      const response = await fetch('https://api.openai.com/v1/audio/speech', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${OPENAI_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: 'tts-1',
          input: text,
          voice: 'nova'
        })
      });
      
      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      
      return new Promise((resolve) => {
        audio.onended = () => {
          URL.revokeObjectURL(audioUrl);
          resolve();
        };
        audio.play();
      });
      
    } catch (error) {
      // Fallback to browser speech synthesis
      return new Promise((resolve) => {
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.onend = resolve;
        speechSynthesis.speak(utterance);
      });
    }
  }
}</code></pre>
                </div>
                <button class="run-btn" onclick="enableFullAssistant()">Build Complete Assistant</button>
            </div>

            <div class="go-deeper">
                <h4>üìö Go Deeper</h4>
                <p><a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API" target="_blank">Web Speech API</a> ‚Äî Browser built-in speech capabilities</p>
                <p><a href="https://aws.amazon.com/polly/" target="_blank">Amazon Polly</a> ‚Äî Enterprise TTS service</p>
            </div>
        </div>

        <!-- The Finale -->
        <div class="chapter" id="finale">
            <h2>Your Agent Has a Voice</h2>
            
            <div class="card">
                <p><strong>You just built the voice interface behind every smart speaker and virtual assistant:</strong></p>
                <ul style="margin: 20px 0; padding-left: 40px;">
                    <li>‚úÖ <strong>Audio capture</strong> ‚Äî Converting sound waves to numbers</li>
                    <li>‚úÖ <strong>Speech recognition</strong> ‚Äî Whisper turning speech into text</li>
                    <li>‚úÖ <strong>AI reasoning</strong> ‚Äî LLM processing and responding</li>
                    <li>‚úÖ <strong>Text-to-speech</strong> ‚Äî Converting responses to audio</li>
                    <li>‚úÖ <strong>Full conversation loop</strong> ‚Äî Natural back-and-forth dialogue</li>
                </ul>
                
                <p>This isn't a prototype. <strong>This is how Alexa works.</strong> How Siri works. How every voice assistant that you've ever used actually processes your voice.</p>
                
                <p><strong>The difference between typing and talking to AI? Just the input and output methods.</strong> The reasoning is identical.</p>
            </div>

            <div class="cta-section">
                <h3>What You Can Build Now</h3>
                <p>With voice, your AI becomes truly conversational.</p>
                <div class="feature-grid">
                    <div class="feature-card">
                        <strong>üè† Smart Home Hub</strong><br>
                        "Turn off the lights and set the temperature to 72"
                    </div>
                    <div class="feature-card">
                        <strong>üìö Learning Companion</strong><br>
                        Practice languages through natural conversation
                    </div>
                    <div class="feature-card">
                        <strong>‚ôø Accessibility Tool</strong><br>
                        Voice control for hands-free computing
                    </div>
                    <div class="feature-card">
                        <strong>üìû Phone Agent</strong><br>
                        Handle calls, take messages, schedule meetings
                    </div>
                    <div class="feature-card">
                        <strong>üéÆ Game Character</strong><br>
                        NPCs that actually understand and respond naturally
                    </div>
                    <div class="feature-card">
                        <strong>üßë‚Äç‚öïÔ∏è Health Assistant</strong><br>
                        Voice-activated symptom checker and medication reminders
                    </div>
                </div>
                <p><strong>Voice interfaces make AI feel magical.</strong></p>
                <p><strong>What conversations will you enable?</strong></p>
            </div>
        </div>
    </div>

    <script>
        // Global state
        let progress = {
            audioEnabled: false,
            speechRecognitionEnabled: false,
            voiceChatEnabled: false,
            ttsEnabled: false,
            fullAssistantEnabled: false
        };
        
        let selectedVoice = 'onyx';
        let isRecording = false;
        let mediaRecorder = null;
        let audioChunks = [];
        let generatedAudio = null;

        // Load saved progress
        const savedProgress = localStorage.getItem('voiceAssistantProgress');
        if (savedProgress) {
            progress = { ...progress, ...JSON.parse(savedProgress) };
        }

        function saveProgress() {
            localStorage.setItem('voiceAssistantProgress', JSON.stringify(progress));
        }

        // Chapter visibility with intersection observer
        document.addEventListener('DOMContentLoaded', function() {
            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('visible');
                    }
                });
            }, { threshold: 0.1 });

            document.querySelectorAll('.chapter').forEach(chapter => {
                observer.observe(chapter);
            });

            updateChapterVisibility();
        });

        function updateChapterVisibility() {
            const chapters = ['chapter-2', 'chapter-3', 'chapter-4', 'chapter-5', 'finale'];
            const requirements = [
                true, // Chapter 2 always visible
                progress.speechRecognitionEnabled,
                progress.voiceChatEnabled,
                progress.ttsEnabled,
                progress.fullAssistantEnabled
            ];

            chapters.forEach((chapterId, index) => {
                const chapter = document.getElementById(chapterId);
                if (requirements[index]) {
                    chapter.classList.add('visible');
                }
            });
        }

        function copyCode(button) {
            const codeBlock = button.nextElementSibling.textContent;
            navigator.clipboard.writeText(codeBlock).then(() => {
                button.textContent = '‚úì Copied';
                setTimeout(() => button.textContent = 'Copy', 2000);
            });
        }

        async function startRecording() {
            if (isRecording) return;
            
            const btn = document.getElementById('record-btn');
            const status = document.getElementById('record-status');
            
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                audioChunks = [];
                
                mediaRecorder.ondataavailable = (event) => {
                    audioChunks.push(event.data);
                };
                
                mediaRecorder.onstop = () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    visualizeAudio(audioBlob);
                    stream.getTracks().forEach(track => track.stop());
                };
                
                mediaRecorder.start();
                isRecording = true;
                
                btn.classList.add('recording');
                status.textContent = 'Recording... (3 seconds)';
                
                // Record for 3 seconds
                setTimeout(() => {
                    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                        mediaRecorder.stop();
                        isRecording = false;
                        btn.classList.remove('recording');
                        status.textContent = 'Processing audio...';
                    }
                }, 3000);
                
            } catch (error) {
                console.error('Recording failed:', error);
                status.textContent = 'Microphone access denied';
            }
        }

        function visualizeAudio(audioBlob) {
            // Show waveform
            const waveform = document.getElementById('waveform');
            waveform.style.display = 'flex';
            waveform.innerHTML = '';
            
            // Create fake waveform bars
            for (let i = 0; i < 50; i++) {
                const bar = document.createElement('div');
                bar.className = 'waveform-bar';
                bar.style.height = Math.random() * 80 + 20 + 'px';
                waveform.appendChild(bar);
            }
            
            // Show spectrogram
            document.getElementById('spectrogram').style.display = 'flex';
            
            // Show audio data
            const audioData = document.getElementById('audio-data');
            audioData.style.display = 'block';
            
            // Simulate audio numbers
            const fakeNumbers = [];
            for (let i = 0; i < 100; i++) {
                fakeNumbers.push((Math.random() - 0.5).toFixed(4));
            }
            audioData.textContent = `Sample audio data: [${fakeNumbers.slice(0, 20).join(', ')}...]`;
            
            document.getElementById('record-status').textContent = '‚úì Audio captured and visualized!';
        }

        function enableAudioCapture() {
            progress.audioEnabled = true;
            const button = event.target;
            button.textContent = '‚úì Audio Capture Enabled';
            button.style.background = '#16a34a';
            updateChapterVisibility();
            saveProgress();
        }

        function animatePipeline() {
            const nodes = ['pipe-audio', 'pipe-spec', 'pipe-encoder', 'pipe-decoder', 'pipe-text'];
            
            nodes.forEach((nodeId, index) => {
                setTimeout(() => {
                    // Remove active from all nodes
                    nodes.forEach(id => document.getElementById(id).classList.remove('active'));
                    // Add active to current node
                    document.getElementById(nodeId).classList.add('active');
                }, index * 800);
            });
            
            // Clear all active states after animation
            setTimeout(() => {
                nodes.forEach(id => document.getElementById(id).classList.remove('active'));
            }, nodes.length * 800 + 1000);
        }

        async function startListening() {
            const btn = document.getElementById('listen-btn');
            const status = document.getElementById('listen-status');
            const transcription = document.getElementById('transcription');
            
            btn.classList.add('recording');
            status.textContent = 'Listening...';
            transcription.style.display = 'block';
            transcription.textContent = 'Listening for speech...';
            
            try {
                // Try browser speech recognition
                if ('webkitSpeechRecognition' in window) {
                    const recognition = new webkitSpeechRecognition();
                    recognition.continuous = false;
                    recognition.interimResults = true;
                    
                    recognition.onresult = (event) => {
                        const result = event.results[0];
                        transcription.textContent = result[0].transcript;
                        
                        if (result.isFinal) {
                            showWordTimings(result[0].transcript);
                        }
                    };
                    
                    recognition.onerror = () => {
                        transcription.textContent = 'Speech recognition failed. This is a simulated example: "Hello, how are you today?"';
                        showWordTimings('Hello, how are you today?');
                    };
                    
                    recognition.onend = () => {
                        btn.classList.remove('recording');
                        status.textContent = 'Click to start listening';
                    };
                    
                    recognition.start();
                    
                } else {
                    // Fallback simulation
                    setTimeout(() => {
                        transcription.textContent = 'Speech recognition not supported. Simulated result: "Hello, how are you today?"';
                        showWordTimings('Hello, how are you today?');
                        btn.classList.remove('recording');
                        status.textContent = 'Click to start listening';
                    }, 2000);
                }
                
            } catch (error) {
                transcription.textContent = 'Error: ' + error.message;
                btn.classList.remove('recording');
                status.textContent = 'Click to start listening';
            }
        }

        function showWordTimings(text) {
            const words = text.split(' ');
            const timingDisplay = document.getElementById('timing-display');
            document.getElementById('word-timing').style.display = 'block';
            
            let timingsText = '';
            words.forEach((word, index) => {
                const start = (index * 0.5).toFixed(1);
                const end = ((index + 1) * 0.5).toFixed(1);
                timingsText += `"${word}": ${start}s - ${end}s\n`;
            });
            
            timingDisplay.textContent = timingsText;
        }

        function enableSpeechRecognition() {
            progress.speechRecognitionEnabled = true;
            const button = event.target;
            button.textContent = '‚úì Speech Recognition Enabled';
            button.style.background = '#16a34a';
            updateChapterVisibility();
            saveProgress();
        }

        function startVoiceChat() {
            const conversation = document.getElementById('voice-conversation');
            conversation.style.display = 'block';
            
            // Add first message
            addMessage('ai', 'Voice chat enabled! This is a simulation of the voice ‚Üí text ‚Üí AI ‚Üí text pipeline.');
            addMessage('processing', 'In a real implementation, your speech would be transcribed, sent to an LLM, and you\'d get an intelligent response.');
            
            document.getElementById('voice-status').textContent = 'Voice chat simulation active';
        }

        function enableVoiceChat() {
            progress.voiceChatEnabled = true;
            startVoiceChat();
            const button = event.target;
            button.textContent = '‚úì Voice Chat Enabled';
            button.style.background = '#16a34a';
            updateChapterVisibility();
            saveProgress();
        }

        function addMessage(type, text) {
            const conversation = document.getElementById('voice-conversation');
            const message = document.createElement('div');
            message.className = `conversation-msg ${type === 'user' ? 'user-voice' : type === 'processing' ? 'processing' : 'ai-voice'}`;
            message.textContent = text;
            conversation.appendChild(message);
            conversation.scrollTop = conversation.scrollHeight;
        }

        function selectVoice(voice) {
            selectedVoice = voice;
            document.querySelectorAll('.voice-option').forEach(option => {
                option.classList.remove('selected');
            });
            document.querySelector(`[data-voice="${voice}"]`).classList.add('selected');
        }

        async function testTextToSpeech() {
            const text = document.getElementById('tts-text').value;
            if (!text.trim()) {
                alert('Please enter some text to speak');
                return;
            }
            
            const result = document.getElementById('tts-result');
            result.style.display = 'flex';
            
            // Simulate TTS (in real app, this would call OpenAI TTS API)
            if ('speechSynthesis' in window) {
                const utterance = new SpeechSynthesisUtterance(text);
                const voices = speechSynthesis.getVoices();
                utterance.voice = voices[0];
                
                generatedAudio = utterance;
                document.querySelector('#tts-result .play-btn').onclick = () => {
                    speechSynthesis.speak(utterance);
                };
                
            } else {
                alert('Text-to-speech not supported in this browser. In a real implementation, you would use the OpenAI TTS API.');
            }
        }

        function playGeneratedAudio() {
            if (generatedAudio && 'speechSynthesis' in window) {
                speechSynthesis.speak(generatedAudio);
            }
        }

        function enableTextToSpeech() {
            progress.ttsEnabled = true;
            const button = event.target;
            button.textContent = '‚úì Text-to-Speech Enabled';
            button.style.background = '#16a34a';
            updateChapterVisibility();
            saveProgress();
        }

        // Full assistant variables
        let fullRecorder = null;
        let isFullListening = false;

        function startFullVoiceChat() {
            if (isFullListening) return;
            
            const btn = document.getElementById('full-talk-btn');
            const status = document.getElementById('full-status');
            
            btn.classList.add('active');
            btn.innerHTML = 'üî¥<br>Recording...';
            status.textContent = 'Listening...';
            isFullListening = true;
            
            // Simulate recording start
            navigator.mediaDevices.getUserMedia({ audio: true })
                .then(stream => {
                    fullRecorder = new MediaRecorder(stream);
                    audioChunks = [];
                    
                    fullRecorder.ondataavailable = (event) => {
                        audioChunks.push(event.data);
                    };
                    
                    fullRecorder.start();
                })
                .catch(error => {
                    console.log('Microphone not available, using simulation');
                });
        }

        function endFullVoiceChat() {
            if (!isFullListening) return;
            
            const btn = document.getElementById('full-talk-btn');
            const status = document.getElementById('full-status');
            const conversation = document.getElementById('full-conversation');
            
            btn.classList.remove('active');
            btn.innerHTML = 'üé§<br>Hold to Talk';
            isFullListening = false;
            
            // Stop recording if active
            if (fullRecorder && fullRecorder.state === 'recording') {
                fullRecorder.stop();
                fullRecorder.stream.getTracks().forEach(track => track.stop());
            }
            
            // Simulate full pipeline
            status.textContent = 'Processing speech...';
            
            setTimeout(() => {
                const userMessage = document.createElement('div');
                userMessage.className = 'conversation-msg user-voice';
                userMessage.textContent = 'What\'s the weather like today?';
                conversation.appendChild(userMessage);
                
                status.textContent = 'Getting AI response...';
                
                setTimeout(() => {
                    const aiMessage = document.createElement('div');
                    aiMessage.className = 'conversation-msg ai-voice';
                    aiMessage.textContent = 'I\'m a demo assistant! In a real implementation, I would check the weather API and give you current conditions for your location.';
                    conversation.appendChild(aiMessage);
                    
                    status.textContent = 'Converting to speech...';
                    
                    setTimeout(() => {
                        // Play audio (browser TTS as fallback)
                        if ('speechSynthesis' in window) {
                            const utterance = new SpeechSynthesisUtterance(aiMessage.textContent);
                            speechSynthesis.speak(utterance);
                        }
                        
                        status.textContent = 'Ready for voice conversation';
                        conversation.scrollTop = conversation.scrollHeight;
                    }, 1000);
                }, 2000);
            }, 1500);
        }

        function enableFullAssistant() {
            progress.fullAssistantEnabled = true;
            const button = event.target;
            button.textContent = '‚úì Complete Assistant Built';
            button.style.background = '#16a34a';
            updateChapterVisibility();
            saveProgress();
        }

        // Initialize on load
        document.addEventListener('DOMContentLoaded', updateChapterVisibility);

        // Prevent text selection during touch events on buttons
        document.addEventListener('selectstart', (e) => {
            if (e.target.classList.contains('talk-button')) {
                e.preventDefault();
            }
        });
    </script>

    <div style="max-width:720px;margin:40px auto;padding:20px;display:flex;justify-content:space-between;font-size:15px;">
        <a href="../build-vision/" style="color:#2563eb;text-decoration:none;">‚Üê Previous: Vision Model</a>
        <a href="../build-eval/" style="color:#2563eb;text-decoration:none;">Next: Eval System ‚Üí</a>
    </div>
</body>
</html>